{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import spacy\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=\"AIzaSyBUG_6Y27wVVMLLTE7LFAD3Fs6NTGje2z0\")\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# NLP model for Spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list of sentences from dataset with duplicates eliminated \n",
    "# input format: [[int1,string],[int2,string],[int3,string]]\n",
    "# output format: [sentence1,sentence2,sent....]\n",
    "def process_data_from_csv(csv_reader):\n",
    "    seen_sentences = set()\n",
    "    result = []\n",
    "\n",
    "    def get_sentences(text):\n",
    "        sentences = text.split('. ')\n",
    "        if sentences and sentences[-1].endswith('.'):\n",
    "            sentences[-1] = sentences[-1][:-1]\n",
    "        return sentences\n",
    "\n",
    "    next(csv_reader, None)  # Skip header if present\n",
    "\n",
    "    for row in csv_reader:\n",
    "        text = row[1]  # second column contains the text\n",
    "\n",
    "        sentences = get_sentences(text)\n",
    "        for sentence in sentences:\n",
    "            if sentence not in seen_sentences:\n",
    "                seen_sentences.add(sentence)\n",
    "                result.append(sentence)  \n",
    "\n",
    "    return result\n",
    "\n",
    "# Filters sentences by contextual relevance to keyword groups.\n",
    "# Input - data: List of sentences, e.g., [\"sentence1\", \"sentence2\", ...]\n",
    "# Input - keywords: List of keyword groups, e.g., [[\"word1\", \"word2\"], [\"word3\"]]\n",
    "# Output: List of lists, where each sublist corresponds to a keyword group.\n",
    "#         Each sublist contains sentences matching the keywords of that group,\n",
    "#         ensuring no sentence is repeated across different groups.\n",
    "#         Non-matching positions within each group are filled with an empty string to maintain list consistency.\n",
    "def filter_context_related_sentences(data, keyword_groups):\n",
    "    # Initialize a list of lists for storage, corresponding to each keyword group\n",
    "    categorized_sentences = [[] for _ in keyword_groups]\n",
    "    target_tokens_groups = [[nlp(keyword) for keyword in group] for group in keyword_groups]\n",
    "\n",
    "    # Process each sentence in the data\n",
    "    count=0\n",
    "    for sentence in data:\n",
    "        print(\"Evaluating sentence\",count,\"...\\n\")\n",
    "        count += 1\n",
    "        doc = nlp(sentence)\n",
    "        # Track which categories the sentence belongs to\n",
    "        matched_indices = []\n",
    "\n",
    "        for group_index, target_tokens in enumerate(target_tokens_groups):\n",
    "            found = False\n",
    "            for token in doc:\n",
    "                for target_token in target_tokens:\n",
    "                    if token.similarity(target_token) > 0.8:\n",
    "                        matched_indices.append(group_index)\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "        # Add the sentence to the matched categories\n",
    "        for index in matched_indices:\n",
    "            categorized_sentences[index].append(sentence)\n",
    "\n",
    "    # Transpose the lists to align sentences across categories without gaps\n",
    "    max_length = max(len(lst) for lst in categorized_sentences)\n",
    "    for lst in categorized_sentences:\n",
    "        lst.extend([\"\"] * (max_length - len(lst)))  # Ensure all lists have the same length\n",
    "\n",
    "    # Combine the lists such that there are no empty entries horizontally\n",
    "    filtered_data = []\n",
    "    for i in range(max_length):\n",
    "        row = [categorized_sentences[group_index][i] for group_index in range(len(keyword_groups))]\n",
    "        filtered_data.append(row)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# class to raise custom error\n",
    "class InvalidInput(Exception):\n",
    "\n",
    "    def __init__(self, message=\"Invalid input, restart program and re-enter\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "\n",
    "# function to choose .csv(dataset file) via a dialog box\n",
    "def choose_csv_file():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  \n",
    "\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select a CSV file\",\n",
    "        filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "    )\n",
    "    if file_path: \n",
    "        print(f\"File selected: {file_path}\")\n",
    "    else:\n",
    "        print(\"No file was selected.\")\n",
    "\n",
    "    return file_path\n",
    "\n",
    "def split_data(data, num_parts):\n",
    "    length = len(data)\n",
    "    return [data[i*length // num_parts: (i+1)*length // num_parts] for i in range(num_parts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "# Input number of categories for data extraction\n",
    "numCategories = int(input(\"How many categories would you like your data extracted into? \"))\n",
    "\n",
    "# Input keyword(s) for each category\n",
    "for x in range(numCategories):\n",
    "    category = input(\"Enter keyword(s) for category \"+ str(x+1) + \" seperated by commas\")\n",
    "    category = category.split(\",\")\n",
    "    keywords.append(category)\n",
    "\n",
    "# Ensure user has left input empty \n",
    "if(keywords ==[] or [\"\"] in keywords):\n",
    "    raise InvalidInput()\n",
    "\n",
    "# Select input/dataset/.csv file\n",
    "selected_file = choose_csv_file()\n",
    "\n",
    "# Create list of column headings for output .csv file\n",
    "column_headings = []\n",
    "for category in keywords:\n",
    "    column_headings.append(category[0])\n",
    "\n",
    "# read raw data from .csv/dataset\n",
    "with open(selected_file, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader)  \n",
    "    data = process_data_from_csv(csv_reader) \n",
    "\n",
    "# Split data into 4 chunks\n",
    "data_chunks = split_data(data, 16)\n",
    "\n",
    "# Create a process pool with 4 workers\n",
    "pool = multiprocessing.Pool(processes=16)\n",
    "\n",
    "# Map the filter_context_related_sentences function to the data chunks directly\n",
    "results = pool.starmap(filter_context_related_sentences, [(chunk, keywords) for chunk in data_chunks])\n",
    "\n",
    "# Close the pool and wait for the work to finish\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\n",
    "#filtered_data = filter_context_related_sentences(data,keywords)\n",
    "\n",
    "with open('clean_dataset.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(column_headings)\n",
    "    for result in results:\n",
    "        writer.writerows(result)\n",
    "\n",
    "print(\"Data has been written to clean_dataset.csv successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essay Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Temperature-0: Length 206652 \n",
      "\n",
      "Request Atmosphere-0: Length 500053 \n",
      "Request Atmosphere-1: Length 62923 \n",
      "\n",
      "\n",
      "Request final: Length 7183 \n",
      "\n",
      "## The Extreme Heat and Dense Atmosphere of Venus: A Study in Planetary Contrasts\n",
      "\n",
      "Venus, often referred to as Earth's \"twin\" due to its similar size and density, presents a stark contrast to our own planet. While Earth teems with life, Venus is an inferno, shrouded in a thick, toxic atmosphere that traps heat in a runaway greenhouse effect, rendering its surface the hottest in our solar system. This essay will delve into the fascinating and extreme characteristics of Venus's atmosphere and temperature, exploring the challenges they pose to exploration and the scientific insights they provide.\n",
      "\n",
      "Venus's atmosphere, composed primarily of carbon dioxide (96.5%), is 92 times denser than Earth's, creating a crushing pressure equivalent to being 900 meters underwater. This thick blanket of gas traps heat from the Sun, driving surface temperatures to a scorching 460 degrees Celsius (860 degrees Fahrenheit). The heat is so intense that it can melt lead, making the surface of Venus inhospitable to any known life forms. This extreme heat is a consequence of the runaway greenhouse effect, a process where the atmosphere's composition and density prevent heat from escaping back into space. This phenomenon, magnified on Venus due to its dense CO2 atmosphere, offers a crucial lesson in understanding the potential consequences of climate change on Earth.\n",
      "\n",
      "Despite the harsh conditions, Venus's atmosphere exhibits unique features that captivate scientists. Its sulfuric acid clouds, which reflect sunlight, contribute significantly to Venus's high albedo, making it the brightest planet in our sky. These clouds also generate sulfuric acid rain, a testament to the planet's corrosive and acidic environment. The slow rotation of Venus, taking 243 Earth days to complete one rotation, leads to unique atmospheric circulation patterns and a near-constant temperature across the planet's surface.\n",
      "\n",
      "Exploring Venus presents immense challenges due to its extreme temperature, pressure, and toxic atmosphere. Spacecraft designed for such harsh conditions are required, with specialized technologies needed to withstand the heat, pressure, and corrosive nature of the environment. This has made human exploration of Venus virtually impossible. Despite the difficulties, unmanned missions continue to provide valuable data on the planet's atmosphere, surface, and potential for past or present life.\n",
      "\n",
      "The study of Venus offers significant scientific value. It provides insights into the evolution of planetary atmospheres, highlighting the potential consequences of runaway greenhouse effects. Moreover, it serves as a cautionary tale for Earth's future, demonstrating the potential impact of unchecked greenhouse gas emissions. Examining the potential for past or present life on Venus, especially in its upper atmosphere cloud layers where conditions are slightly less extreme, remains a captivating field of study.\n",
      "\n",
      "In conclusion, Venus stands as a testament to the diversity and extremes within our solar system. Its incredibly hot and dense atmosphere, though inhospitable to life as we know it, provides a valuable laboratory for understanding the complexities of planetary evolution, climate change, and the potential for life beyond Earth. While the challenges of exploring Venus remain significant, the scientific rewards are immense, driving ongoing research and inspiring future missions to unravel the mysteries of this fascinating and enigmatic world.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_content_chunk(text_chunk, prompt_prefix, request_id):\n",
    "    prompt = f\"{prompt_prefix}\\n{text_chunk}\"\n",
    "    print(f\"Request {request_id}: Length {len(prompt)} \\n\")\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "def read_csv_and_split_columns(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    column_texts = {col: ' '.join(df[col].dropna().astype(str).tolist()) for col in df.columns}\n",
    "    return column_texts\n",
    "\n",
    "def split_text(text, max_length=500000):#161000):\n",
    "    return [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "def process_column_data(text, column_name, executor, column_index):\n",
    "    prompt_prefix = f\"Generate a list of points about {column_name} of Venus.\"\n",
    "    chunks = split_text(text)\n",
    "    futures = [executor.submit(generate_content_chunk, chunk, prompt_prefix, f\"{column_name}-{i}\") for i, chunk in enumerate(chunks)]\n",
    "    results = [future.result() for future in futures]\n",
    "    return '\\n'.join(results)\n",
    "\n",
    "def main(csv_file):\n",
    "    column_texts = read_csv_and_split_columns(csv_file)\n",
    "    with ThreadPoolExecutor(max_workers=len(column_texts)) as executor:\n",
    "        results = {col: process_column_data(text, col, executor, i) for i, (col, text) in enumerate(column_texts.items())}\n",
    "    \n",
    "    # Combine all results and generate the final essay\n",
    "    combined_text = \" \".join([f\"{col}: {text}\" for col, text in results.items()])\n",
    "    final_prompt = \"Write an essay about \" + \", \".join(column_texts.keys()) + \" of Venus.\"\n",
    "    final_essay = generate_content_chunk(combined_text, final_prompt, \"final\")\n",
    "    print(final_essay)\n",
    "\n",
    "# Example of usage\n",
    "csv_file_path = \"clean_dataset.csv\"\n",
    "main(csv_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
