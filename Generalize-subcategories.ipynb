{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import spacy\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=\"AIzaSyBUG_6Y27wVVMLLTE7LFAD3Fs6NTGje2z0\")\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# NLP model for Spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list of sentences from dataset with duplicates eliminated \n",
    "# input format: [[int1,string],[int2,string],[int3,string]]\n",
    "# output format: [sentence1,sentence2,sent....]\n",
    "def process_data_from_csv(csv_reader):\n",
    "    seen_sentences = set()\n",
    "    result = []\n",
    "\n",
    "    def get_sentences(text):\n",
    "        sentences = text.split('. ')\n",
    "        if sentences and sentences[-1].endswith('.'):\n",
    "            sentences[-1] = sentences[-1][:-1]\n",
    "        return sentences\n",
    "\n",
    "    next(csv_reader, None)  # Skip header if present\n",
    "\n",
    "    for row in csv_reader:\n",
    "        text = row[1]  # second column contains the text\n",
    "\n",
    "        sentences = get_sentences(text)\n",
    "        for sentence in sentences:\n",
    "            if sentence not in seen_sentences:\n",
    "                seen_sentences.add(sentence)\n",
    "                result.append(sentence)  \n",
    "\n",
    "    return result\n",
    "\n",
    "# Filters sentences by contextual relevance to keyword groups.\n",
    "# Input - data: List of sentences, e.g., [\"sentence1\", \"sentence2\", ...]\n",
    "# Input - keywords: List of keyword groups, e.g., [[\"word1\", \"word2\"], [\"word3\"]]\n",
    "# Output: List of lists, where each sublist corresponds to a keyword group.\n",
    "#         Each sublist contains sentences matching the keywords of that group,\n",
    "#         ensuring no sentence is repeated across different groups.\n",
    "#         Non-matching positions within each group are filled with an empty string to maintain list consistency.\n",
    "def filter_context_related_sentences(data, keyword_groups):\n",
    "    # Initialize a list of lists for storage, corresponding to each keyword group\n",
    "    categorized_sentences = [[] for _ in keyword_groups]\n",
    "    target_tokens_groups = [[nlp(keyword) for keyword in group] for group in keyword_groups]\n",
    "\n",
    "    # Process each sentence in the data\n",
    "    count=0\n",
    "    for sentence in data:\n",
    "        print(\"Evaluating sentence\",count,\"...\")\n",
    "        count += 1\n",
    "        doc = nlp(sentence)\n",
    "        # Track which categories the sentence belongs to\n",
    "        matched_indices = []\n",
    "\n",
    "        for group_index, target_tokens in enumerate(target_tokens_groups):\n",
    "            found = False\n",
    "            for token in doc:\n",
    "                for target_token in target_tokens:\n",
    "                    if token.similarity(target_token) > 0.8:\n",
    "                        matched_indices.append(group_index)\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "        # Add the sentence to the matched categories\n",
    "        for index in matched_indices:\n",
    "            categorized_sentences[index].append(sentence)\n",
    "\n",
    "    # Transpose the lists to align sentences across categories without gaps\n",
    "    max_length = max(len(lst) for lst in categorized_sentences)\n",
    "    for lst in categorized_sentences:\n",
    "        lst.extend([\"\"] * (max_length - len(lst)))  # Ensure all lists have the same length\n",
    "\n",
    "    # Combine the lists such that there are no empty entries horizontally\n",
    "    filtered_data = []\n",
    "    for i in range(max_length):\n",
    "        row = [categorized_sentences[group_index][i] for group_index in range(len(keyword_groups))]\n",
    "        filtered_data.append(row)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# class to raise custom error\n",
    "class InvalidInput(Exception):\n",
    "\n",
    "    def __init__(self, message=\"Invalid input, restart program and re-enter\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "\n",
    "# function to choose .csv(dataset file) via a dialog box\n",
    "def choose_csv_file():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  \n",
    "\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select a CSV file\",\n",
    "        filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "    )\n",
    "    if file_path: \n",
    "        print(f\"File selected: {file_path}\")\n",
    "    else:\n",
    "        print(\"No file was selected.\")\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "# Input number of categories for data extraction\n",
    "numCategories = int(input(\"How many categories would you like your data extracted into? \"))\n",
    "\n",
    "# Input keyword(s) for each category\n",
    "for x in range(numCategories):\n",
    "    category = input(\"Enter keyword(s) for category \"+ str(x+1) + \" seperated by commas\")\n",
    "    category = category.split(\",\")\n",
    "    keywords.append(category)\n",
    "\n",
    "# Ensure user has left input empty \n",
    "if(keywords ==[] or [\"\"] in keywords):\n",
    "    raise InvalidInput()\n",
    "\n",
    "# Select input/dataset/.csv file\n",
    "selected_file = choose_csv_file()\n",
    "\n",
    "# Create list of column headings for output .csv file\n",
    "column_headings = []\n",
    "for category in keywords:\n",
    "    column_headings.append(category[0])\n",
    "\n",
    "# read raw data from .csv/dataset\n",
    "with open(selected_file, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader)  \n",
    "    data = process_data_from_csv(csv_reader) \n",
    "\n",
    "filtered_data = filter_context_related_sentences(data,keywords)\n",
    "\n",
    "with open('clean_dataset.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(column_headings)  \n",
    "    writer.writerows(filtered_data)\n",
    "\n",
    "print(\"Data has been written to clean_dataset.csv successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Temperature-0: Length 276668 \n",
      "\n",
      "Request Atmosphere-0: Length 500053 \n",
      "\n",
      "Request Atmosphere-1: Length 158700 \n",
      "\n",
      "Request Weather Patterns-0: Length 103122 \n",
      "\n",
      "Request final: Length 11641 \n",
      "\n",
      "## The Unforgiving Beauty: Understanding Venus's Temperature, Atmosphere, and Weather Patterns\n",
      "\n",
      "Venus, often dubbed Earth's \"sister planet,\" is a world of stark contrasts. Though similar in size and composition to our own, it harbors a surface that is a crucible of extreme heat, crushing pressure, and a toxic atmosphere.  Understanding Venus, therefore, necessitates delving into its unforgiving climate, marked by a runaway greenhouse effect and a unique set of weather patterns. \n",
      "\n",
      "The most striking feature of Venus's climate is its extreme heat.  Surface temperatures soar to an astonishing 464°C (867°F), hot enough to melt lead. This scorching environment is a direct consequence of the planet's dense atmosphere, primarily composed of carbon dioxide (96.5%).  This thick blanket of gas traps solar radiation, creating a runaway greenhouse effect far more intense than on Earth.  Adding to the infernal conditions are the planet's sulfuric acid clouds, reflecting sunlight back to the surface and contributing to the planet's extreme temperatures.  The combination of these factors creates a hostile environment where no known life could survive.\n",
      "\n",
      "While Venus's atmosphere is dense and opaque, obscuring our view of the surface, it is also a dynamic one.  High wind speeds, reaching up to 240 miles per hour, sweep across the planet's upper atmosphere. These winds are driven by the intense solar radiation absorbed by the atmosphere and the planet's slow rotation, taking 243 Earth days to complete a single turn.  This slow rotation, coupled with its retrograde spin, makes Venus an anomaly in our solar system.\n",
      "\n",
      "One of the most intriguing aspects of Venus is the possibility of past habitability.  Though currently a desolate landscape, evidence suggests that Venus may have once harbored liquid water, a crucial ingredient for life as we know it.  Signs of ancient riverbeds and lakes hint at a time when Venus might have been more Earth-like, with a potentially habitable climate. \n",
      "\n",
      "Studying Venus offers invaluable insights into planetary evolution and climate change. The runaway greenhouse effect on Venus serves as a cautionary tale for Earth, highlighting the potential dangers of unchecked greenhouse gas emissions.  Understanding the factors that led to Venus's extreme climate can inform our efforts to mitigate climate change on our own planet. \n",
      "\n",
      "However, exploring Venus presents significant challenges.  The extreme heat, crushing pressure, and toxic atmosphere make it one of the most difficult planets to study.  Despite these hurdles, scientists continue to push the boundaries of technology, sending spacecraft like Magellan, Venus Express, and Akatsuki to probe its secrets.  Future missions, armed with advanced instruments and innovative technologies, will strive to unlock the mysteries of this enigmatic world. \n",
      "\n",
      "In conclusion, Venus is a testament to the diverse and complex nature of our solar system.  Its unforgiving climate, marked by extreme heat, a thick toxic atmosphere, and unique weather patterns, presents a stark contrast to Earth.  Despite the challenges, Venus offers a window into planetary evolution, climate change, and the potential for life beyond our own world.  By unraveling the secrets of Venus, we can gain a deeper understanding of our place in the cosmos and the forces that shape our own planet. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_content_chunk(text_chunk, prompt_prefix, request_id):\n",
    "    prompt = f\"{prompt_prefix}\\n{text_chunk}\"\n",
    "    print(f\"Request {request_id}: Length {len(prompt)} \\n\")\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "def read_csv_and_split_columns(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    column_texts = {col: ' '.join(df[col].dropna().astype(str).tolist()) for col in df.columns}\n",
    "    return column_texts\n",
    "\n",
    "def split_text(text, max_length=500000):#161000):\n",
    "    return [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "def process_column_data(text, column_name, executor, column_index):\n",
    "    prompt_prefix = f\"Generate a list of points about {column_name} of Venus.\"\n",
    "    chunks = split_text(text)\n",
    "    futures = [executor.submit(generate_content_chunk, chunk, prompt_prefix, f\"{column_name}-{i}\") for i, chunk in enumerate(chunks)]\n",
    "    results = [future.result() for future in futures]\n",
    "    return '\\n'.join(results)\n",
    "\n",
    "def main(csv_file):\n",
    "    column_texts = read_csv_and_split_columns(csv_file)\n",
    "    with ThreadPoolExecutor(max_workers=len(column_texts)) as executor:\n",
    "        results = {col: process_column_data(text, col, executor, i) for i, (col, text) in enumerate(column_texts.items())}\n",
    "    \n",
    "    # Combine all results and generate the final essay\n",
    "    combined_text = \" \".join([f\"{col}: {text}\" for col, text in results.items()])\n",
    "    final_prompt = \"Write an essay about \" + \", \".join(column_texts.keys()) + \" of Venus.\"\n",
    "    final_essay = generate_content_chunk(combined_text, final_prompt, \"final\")\n",
    "    print(final_essay)\n",
    "\n",
    "# Example of usage\n",
    "csv_file_path = \"clean_dataset.csv\"\n",
    "main(csv_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
